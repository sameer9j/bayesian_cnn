{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "# print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# !pip install pyro-ppl\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Categorical\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "\n",
    "import os\n",
    "\n",
    "# third-party library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_size=(40,40), batch_size=64, root=\"../datasets/MainImageFolder\"):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.ImageFolder(root=root, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return train_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_ld = load_images(root='./train/')\n",
    "train_ld1 = load_images(root='./train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22424\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ld.dataset))\n",
    "validation_split = 0.3\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_ld.dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_same_index(target, label):\n",
    "    label_indices = []\n",
    "    \n",
    "    for i in range(len(target)):\n",
    "        if target[i][1] == label:\n",
    "            label_indices.append(i)\n",
    "\n",
    "    return label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = get_same_index(train_ld.dataset.samples,0)\n",
    "sampler_chk1 = torch.utils.data.sampler.SubsetRandomSampler(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampler_chk1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6022"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx1 = list(set(train_idx) - set(sampler_chk1))\n",
    "print(len(train_idx1))\n",
    "\n",
    "valid_idx1 = list(set(valid_idx) - set(sampler_chk1))\n",
    "len(valid_idx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13913\n",
      "6022\n",
      "2489\n"
     ]
    }
   ],
   "source": [
    "train_data = train_ld.dataset\n",
    "train_data1 = train_ld1.dataset\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx1)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler)\n",
    "print(len(train_loader.sampler))\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler)\n",
    "print(len(valid_loader.sampler))\n",
    "valid_loader_ooc = torch.utils.data.DataLoader(train_data1, batch_size=batch_size, \n",
    "    sampler=sampler_chk1)\n",
    "print(len(valid_loader_ooc.sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size[0])\n",
    "        self.fc2 = nn.Linear(hidden_size[0], hidden_size[1])\n",
    "        self.fc3 = nn.Linear(hidden_size[1], hidden_size[2])\n",
    "        self.out = nn.Linear(hidden_size[2], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.fc3(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dimensions\n",
    "dim = 40*40*3\n",
    "dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 4800])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = NN(dim, [2048,1024,512], 10)\n",
    "net.fc1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(net.fc2.weight.shape)\n",
    "net.fc3.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    \n",
    "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
    "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
    "    \n",
    "    fc2w_prior = Normal(loc=torch.zeros_like(net.fc2.weight), scale=torch.ones_like(net.fc2.weight))\n",
    "    fc2b_prior = Normal(loc=torch.zeros_like(net.fc2.bias), scale=torch.ones_like(net.fc2.bias))\n",
    "    \n",
    "    fc3w_prior = Normal(loc=torch.zeros_like(net.fc3.weight), scale=torch.ones_like(net.fc3.weight))\n",
    "    fc3b_prior = Normal(loc=torch.zeros_like(net.fc3.bias), scale=torch.ones_like(net.fc3.bias))\n",
    "    \n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
    "    \n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,\n",
    "              'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior,\n",
    "              'fc3.weight': fc3w_prior, 'fc3.bias': fc3b_prior,\n",
    "              'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    # sample a regressor (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    \n",
    "    lhat = log_softmax(lifted_reg_model(x_data))\n",
    "    \n",
    "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "\n",
    "def guide(x_data, y_data):\n",
    "    \n",
    "    # First layer weight distribution priors\n",
    "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
    "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
    "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
    "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
    "    \n",
    "    # First layer bias distribution priors\n",
    "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
    "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
    "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
    "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
    "    \n",
    "    # Second layer weight distribution priors\n",
    "    fc2w_mu = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_sigma = torch.randn_like(net.fc2.weight)\n",
    "    fc2w_mu_param = pyro.param(\"fc2w_mu\", fc2w_mu)\n",
    "    fc2w_sigma_param = softplus(pyro.param(\"fc2w_sigma\", fc2w_sigma))\n",
    "    fc2w_prior = Normal(loc=fc2w_mu_param, scale=fc2w_sigma_param)\n",
    "    \n",
    "    # Second layer bias distribution priors\n",
    "    fc2b_mu = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_sigma = torch.randn_like(net.fc2.bias)\n",
    "    fc2b_mu_param = pyro.param(\"fc2b_mu\", fc2b_mu)\n",
    "    fc2b_sigma_param = softplus(pyro.param(\"fc2b_sigma\", fc2b_sigma))\n",
    "    fc2b_prior = Normal(loc=fc2b_mu_param, scale=fc2b_sigma_param)\n",
    "    \n",
    "    # Third layer weight distribution priors\n",
    "    fc3w_mu = torch.randn_like(net.fc3.weight)\n",
    "    fc3w_sigma = torch.randn_like(net.fc3.weight)\n",
    "    fc3w_mu_param = pyro.param(\"fc3w_mu\", fc3w_mu)\n",
    "    fc3w_sigma_param = softplus(pyro.param(\"fc3w_sigma\", fc3w_sigma))\n",
    "    fc3w_prior = Normal(loc=fc3w_mu_param, scale=fc3w_sigma_param)\n",
    "    \n",
    "    # Third layer bias distribution priors\n",
    "    fc3b_mu = torch.randn_like(net.fc3.bias)\n",
    "    fc3b_sigma = torch.randn_like(net.fc3.bias)\n",
    "    fc3b_mu_param = pyro.param(\"fc3b_mu\", fc3b_mu)\n",
    "    fc3b_sigma_param = softplus(pyro.param(\"fc3b_sigma\", fc3b_sigma))\n",
    "    fc3b_prior = Normal(loc=fc3b_mu_param, scale=fc3b_sigma_param)\n",
    "    \n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu = torch.randn_like(net.out.weight)\n",
    "    outw_sigma = torch.randn_like(net.out.weight)\n",
    "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
    "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n",
    "    \n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu = torch.randn_like(net.out.bias)\n",
    "    outb_sigma = torch.randn_like(net.out.bias)\n",
    "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
    "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,\n",
    "              'fc2.weight': fc2w_prior, 'fc2.bias': fc2b_prior,\n",
    "              'fc3.weight': fc3w_prior, 'fc3.bias': fc3b_prior,\n",
    "              'out.weight': outw_prior, 'out.bias': outb_prior}\n",
    "    \n",
    "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
    "    \n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.001})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss  1489335.6984142622\n",
      "Epoch  1  Loss  1237516.193625883\n",
      "Epoch  2  Loss  1003611.1968710787\n",
      "Epoch  3  Loss  880116.3183818151\n",
      "Epoch  4  Loss  732969.9040519302\n",
      "Epoch  5  Loss  651926.0000691535\n",
      "Epoch  6  Loss  600437.33739801\n",
      "Epoch  7  Loss  530023.2363073123\n",
      "Epoch  8  Loss  498190.60210318776\n",
      "Epoch  9  Loss  440688.44095772255\n",
      "Epoch  10  Loss  409508.4472975816\n",
      "Epoch  11  Loss  380744.7476058939\n",
      "Epoch  12  Loss  363521.89607579017\n",
      "Epoch  13  Loss  354797.25308985024\n",
      "Epoch  14  Loss  310327.9889377143\n",
      "Epoch  15  Loss  291276.377021278\n",
      "Epoch  16  Loss  271320.104035192\n",
      "Epoch  17  Loss  257017.28685821465\n",
      "Epoch  18  Loss  235875.2528550348\n",
      "Epoch  19  Loss  234616.64019719366\n",
      "Epoch  20  Loss  220664.59948457216\n",
      "Epoch  21  Loss  210385.6606556886\n",
      "Epoch  22  Loss  197864.50801952137\n",
      "Epoch  23  Loss  186949.90856290964\n",
      "Epoch  24  Loss  178810.3914751425\n",
      "Epoch  25  Loss  171116.93146307766\n",
      "Epoch  26  Loss  160378.92801496605\n",
      "Epoch  27  Loss  148744.23368488223\n",
      "Epoch  28  Loss  143715.04943129583\n",
      "Epoch  29  Loss  136333.43935090324\n",
      "Epoch  30  Loss  130534.55296273924\n",
      "Epoch  31  Loss  122083.10529923749\n",
      "Epoch  32  Loss  119551.05925043038\n",
      "Epoch  33  Loss  113300.94112079796\n",
      "Epoch  34  Loss  107659.02772889457\n",
      "Epoch  35  Loss  102750.21026003922\n",
      "Epoch  36  Loss  96857.20248198803\n",
      "Epoch  37  Loss  91664.55009203075\n",
      "Epoch  38  Loss  90205.33727930838\n",
      "Epoch  39  Loss  84727.29250309485\n",
      "Epoch  40  Loss  80962.67709875679\n",
      "Epoch  41  Loss  77034.08507284784\n",
      "Epoch  42  Loss  74839.9875720898\n",
      "Epoch  43  Loss  71073.4368523499\n",
      "Epoch  44  Loss  66880.93636489661\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 50\n",
    "loss = 0\n",
    "loss_tr = []\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = 0\n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss += svi.step(data[0].view(-1,dim), data[1])\n",
    "    normalizer_train = len(train_loader.dataset)\n",
    "    total_epoch_loss_train = loss / normalizer_train\n",
    "    loss_tr.append(total_epoch_loss_train)\n",
    "    \n",
    "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_tr, label='Training loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n",
      "accuracy: 25 %\n"
     ]
    }
   ],
   "source": [
    "num_samples = 100\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.numpy(), axis=1)\n",
    "\n",
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data in enumerate(valid_loader):\n",
    "    images, labels = data\n",
    "    predicted = predict(images.view(-1,dim))\n",
    "    total += labels.size(0)\n",
    "#     print((predicted))\n",
    "#     print(labels.numpy())\n",
    "    correct += (predicted == labels.numpy()).sum().item()\n",
    "    break\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 1\n",
    "\n",
    "# print('Prediction when network is forced to predict')\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for j, data in enumerate(valid_loader):\n",
    "#     images, labels = data\n",
    "#     predicted = predict(images.view(-1,dim))\n",
    "#     total += labels.size(0)\n",
    "# #     print((predicted))\n",
    "# #     print(labels.numpy())\n",
    "#     correct += (predicted == labels.numpy()).sum().item()\n",
    "#     break\n",
    "# print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 100\n",
    "\n",
    "# print('Prediction when network is forced to predict')\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# for j, data in enumerate(valid_loader_ooc):\n",
    "#     images, labels = data\n",
    "#     predicted = predict(images.view(-1,dim))\n",
    "#     total += labels.size(0)\n",
    "# #     print((predicted))\n",
    "# #     print(labels.numpy())\n",
    "#     correct += (predicted == labels.numpy()).sum().item()\n",
    "#     break\n",
    "# print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    #plt.imshow(npimg,  cmap='gray')\n",
    "    #fig.show(figsize=(1,1))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(1, 1))\n",
    "    ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "def give_uncertainities(x):\n",
    "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
    "    yhats = [F.log_softmax(model(x.view(-1,dim)).data, 1).detach().numpy() for model in sampled_models]\n",
    "    return np.asarray(yhats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = ('c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9')\n",
    "classes = ('1', '2', '3', '4', '5', '6', '7', '8', '9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(images, labels, plot=True):\n",
    "    y = give_uncertainities(images)\n",
    "    predicted_for_images = 0\n",
    "    correct_predictions=0\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "    \n",
    "        if(plot):\n",
    "            print(\"Real: \",labels[i].item())\n",
    "            fig, axs = plt.subplots(1, 9, sharey=True,figsize=(20,2))\n",
    "    \n",
    "        all_digits_prob = []\n",
    "    \n",
    "        highted_something = False\n",
    "    \n",
    "        for j in range(len(classes)):\n",
    "        \n",
    "            highlight=False\n",
    "        \n",
    "            histo = []\n",
    "            histo_exp = []\n",
    "        \n",
    "            for z in range(y.shape[0]):\n",
    "                histo.append(y[z][i][j])\n",
    "                histo_exp.append(np.exp(y[z][i][j]))\n",
    "            \n",
    "            prob = np.percentile(histo_exp, 50) #sampling median probability\n",
    "        \n",
    "            if(prob>0.1): #select if network thinks this sample is 20% chance of this being a label\n",
    "                highlight = True #possibly an answer\n",
    "        \n",
    "            all_digits_prob.append(prob)\n",
    "            \n",
    "            if(plot):\n",
    "            \n",
    "                N, bins, patches = axs[j].hist(histo, bins=8, color = \"lightgray\", lw=0,  weights=np.ones(len(histo)) / len(histo), density=False)\n",
    "                axs[j].set_title(str(j)+\" (\"+str(round(prob,2))+\")\") \n",
    "        \n",
    "            if(highlight):\n",
    "            \n",
    "                highted_something = True\n",
    "                \n",
    "                if(plot):\n",
    "\n",
    "                    # We'll color code by height, but you could use any scalar\n",
    "                    fracs = N / N.max()\n",
    "\n",
    "                    # we need to normalize the data to 0..1 for the full range of the colormap\n",
    "                    norm = colors.Normalize(fracs.min(), fracs.max())\n",
    "\n",
    "                    # Now, we'll loop through our objects and set the color of each accordingly\n",
    "                    for thisfrac, thispatch in zip(fracs, patches):\n",
    "                        color = plt.cm.viridis(norm(thisfrac))\n",
    "                        thispatch.set_facecolor(color)\n",
    "\n",
    "    \n",
    "        if(plot):\n",
    "            plt.show()\n",
    "    \n",
    "        predicted = np.argmax(all_digits_prob)\n",
    "    \n",
    "        if(highted_something):\n",
    "            predicted_for_images+=1\n",
    "            if(labels[i].item()==predicted):\n",
    "                if(plot):\n",
    "                    print(\"Correct\")\n",
    "                correct_predictions +=1.0\n",
    "            else:\n",
    "                if(plot):\n",
    "                    print(\"Incorrect :()\")\n",
    "        else:\n",
    "            if(plot):\n",
    "                print(\"Undecided.\")\n",
    "        \n",
    "        if(plot):\n",
    "            imshow(images[i].squeeze())\n",
    "        \n",
    "    \n",
    "    if(plot):\n",
    "        print(\"Summary\")\n",
    "        print(\"Total images: \",len(labels))\n",
    "        print(\"Predicted for: \",predicted_for_images)\n",
    "        print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n",
    "        \n",
    "    return len(labels), correct_predictions, predicted_for_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network can refuse\n",
      "Total input images:  6022\n",
      "Skipped images:  5871\n",
      "Accuracy, when attempted predictions: 27 %\n"
     ]
    }
   ],
   "source": [
    "# Prediction when network can decide not to predict\n",
    "\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "for j, data in enumerate(valid_loader):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "\n",
    "print(\"Total input images: \", total)\n",
    "print(\"Skipped images: \", total-total_predicted_for)\n",
    "print(\"Accuracy, when attempted predictions: %d %%\" % (100 * correct / total_predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network can refuse\n",
      "Total images:  2489\n",
      "Skipped images:  2441\n",
      "Accuracy, when attempted predictions: 0 %\n"
     ]
    }
   ],
   "source": [
    "# Prediction when network can decide not to predict\n",
    "\n",
    "print('Prediction when network can refuse')\n",
    "correct = 0\n",
    "total = 0\n",
    "total_predicted_for = 0\n",
    "for j, data in enumerate(valid_loader_ooc):\n",
    "    images, labels = data\n",
    "    \n",
    "    total_minibatch, correct_minibatch, predictions_minibatch = test_batch(images, labels, plot=False)\n",
    "    total += total_minibatch\n",
    "    correct += correct_minibatch\n",
    "    total_predicted_for += predictions_minibatch\n",
    "\n",
    "print(\"Total images: \", total)\n",
    "print(\"Skipped images: \", total-total_predicted_for)\n",
    "print(\"Accuracy, when attempted predictions: %d %%\" % (100 * correct / total_predicted_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preparing for evaluation\n",
    "# dataiter = iter(valid_loader_ooc)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# test_batch(images, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
