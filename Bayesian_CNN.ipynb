{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import datetime\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "#from utils.autoaugment import CIFAR10Policy\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.BBBlayers import GaussianVariationalInference\n",
    "from utils.BayesianModels.Bayesian3Conv3FC import BBB3Conv3FC\n",
    "from utils.BayesianModels.BayesianAlexNet import BBBAlexNet\n",
    "from utils.BayesianModels.BayesianLeNet import BBBLeNet\n",
    "#from utils.BayesianModels.BayesianSqueezeNet import BBBSqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_type = 'alexnet'\n",
    "dataset = 'CIFAR10'\n",
    "outputs = 9\n",
    "inputs = 3\n",
    "resume = False\n",
    "n_epochs = 10\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0005\n",
    "num_samples = 10\n",
    "beta_type = \"Blundell\"\n",
    "resize=80\n",
    "im_size=80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_size=(im_size,im_size), batch_size=128, root=\"D:\\justi\\Documents\\BML Proj\"):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.ImageFolder(root=root, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22424\n",
      "2260\n"
     ]
    }
   ],
   "source": [
    "batch_size = 24\n",
    "train_ld = load_images(root='D:/justi/Documents/BMLProj/train/')\n",
    "train_ld1 = load_images(root='D:/justi/Documents/BMLProj/train/')\n",
    "\n",
    "print(len(train_ld.dataset))\n",
    "validation_split = 0.3\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_ld.dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "def get_same_index(target, label):\n",
    "    label_indices = []\n",
    "    \n",
    "    for i in range(len(target)):\n",
    "        if target[i][1] == label:\n",
    "            label_indices.append(i)\n",
    "\n",
    "    return label_indices\n",
    "\n",
    "ind = get_same_index(train_ld.dataset.samples,1)\n",
    "# del ind[:27]\n",
    "del ind[:7]\n",
    "print(len(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260\n",
      "14139\n",
      "14139\n",
      "6024\n",
      "2260\n"
     ]
    }
   ],
   "source": [
    "sampler_chk1 = torch.utils.data.sampler.SubsetRandomSampler(ind)\n",
    "\n",
    "print(len(sampler_chk1))\n",
    "\n",
    "train_idx1 = list(set(train_idx) - set(sampler_chk1))\n",
    "print(len(train_idx1))\n",
    "\n",
    "valid_idx1 = list(set(valid_idx) - set(sampler_chk1))\n",
    "del valid_idx1[0]\n",
    "len(valid_idx1)\n",
    "\n",
    "train_data = train_ld.dataset\n",
    "train_data1 = train_ld1.dataset\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx1)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler)\n",
    "print(len(train_loader.sampler))\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler)\n",
    "print(len(valid_loader.sampler))\n",
    "valid_loader_ooc = torch.utils.data.DataLoader(train_data1, batch_size=batch_size, \n",
    "    sampler=sampler_chk1)\n",
    "print(len(valid_loader_ooc.sampler))\n",
    "\n",
    "train_data=train_loader.dataset\n",
    "valid_data=valid_loader.dataset\n",
    "valset=valid_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the image classes\n",
    "classes = ['c0', 'c2', 'c3', 'c4',\n",
    "           'c5', 'c6', 'c7', 'c8', 'c9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "if (net_type == 'lenet'):\n",
    "    net = BBBLeNet(outputs,inputs)\n",
    "elif (net_type == 'alexnet'):\n",
    "    net = BBBAlexNet(outputs,inputs)\n",
    "elif (net_type == '3conv3fc'):\n",
    "        net = BBB3Conv3FC(outputs,inputs)\n",
    "else:\n",
    "    print('Error : Network should be either [LeNet / AlexNet / 3Conv3FC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = GaussianVariationalInference(torch.nn.CrossEntropyLoss())\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_alexnet_CIFAR10_bayesian_v6.pt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_name = f'model_{net_type}_{dataset}_bayesian_v6.pt'\n",
    "ckpt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "Epoch: 1 \tTraining Loss: 2563.090078 \tValidation Loss: 2.815736\n",
      "Validation loss decreased (inf --> 2.815736).  Saving model ...\n",
      "hi\n",
      "Epoch: 2 \tTraining Loss: 2442.370456 \tValidation Loss: 2.795225\n",
      "Validation loss decreased (2.815736 --> 2.795225).  Saving model ...\n",
      "hi\n",
      "Epoch: 3 \tTraining Loss: 2280.096706 \tValidation Loss: 2.212632\n",
      "Validation loss decreased (2.795225 --> 2.212632).  Saving model ...\n",
      "hi\n",
      "Epoch: 4 \tTraining Loss: 2110.341430 \tValidation Loss: 1.134389\n",
      "Validation loss decreased (2.212632 --> 1.134389).  Saving model ...\n",
      "hi\n",
      "Epoch: 5 \tTraining Loss: 1938.048094 \tValidation Loss: 0.811361\n",
      "Validation loss decreased (1.134389 --> 0.811361).  Saving model ...\n",
      "hi\n",
      "Epoch: 6 \tTraining Loss: 1772.620871 \tValidation Loss: 0.714316\n",
      "Validation loss decreased (0.811361 --> 0.714316).  Saving model ...\n",
      "hi\n",
      "Epoch: 7 \tTraining Loss: 1609.623717 \tValidation Loss: 0.601214\n",
      "Validation loss decreased (0.714316 --> 0.601214).  Saving model ...\n",
      "hi\n",
      "Epoch: 8 \tTraining Loss: 1451.534469 \tValidation Loss: 0.558117\n",
      "Validation loss decreased (0.601214 --> 0.558117).  Saving model ...\n",
      "hi\n",
      "Epoch: 9 \tTraining Loss: 1300.435197 \tValidation Loss: 0.529859\n",
      "Validation loss decreased (0.558117 --> 0.529859).  Saving model ...\n",
      "hi\n",
      "Epoch: 10 \tTraining Loss: 1159.209017 \tValidation Loss: 0.525098\n",
      "Validation loss decreased (0.529859 --> 0.525098).  Saving model ...\n",
      "Wall time: 1h 18min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    m = math.ceil(len(train_data) / batch_size)\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        data = data.view(-1, inputs, resize, resize).repeat(num_samples, 1, 1, 1)\n",
    "        target = target.repeat(num_samples)\n",
    "        #print(data.shape, target.shape)\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        if beta_type is \"Blundell\":\n",
    "            beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "        elif beta_type is \"Soenderby\":\n",
    "            beta = min(epoch / (n_epochs // 4), 1)\n",
    "        elif beta_type is \"Standard\":\n",
    "            beta = 1 / m\n",
    "        else:\n",
    "            beta = 0\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "\n",
    "        output,kl = net.probforward(data)\n",
    "#         print(\"CNN output :{}\".format(output.shape))\n",
    "        # calculate the batch loss\n",
    "#         print(target.shape)\n",
    "        loss = vi(output, target, kl, beta)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += (loss.item()*data.size(0)) / num_samples\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    net.eval()\n",
    "    print('hi')\n",
    "    for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "        data = data.view(-1, inputs, resize, resize).repeat(num_samples, 1, 1, 1)\n",
    "\n",
    "        target = target.repeat(num_samples)\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output,kl = net.probforward(data)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = vi(output, target, kl, beta)\n",
    "        # update average validation loss \n",
    "        valid_loss += (loss.item()*data.size(0)) / num_samples\n",
    "        \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/(len(train_loader.dataset) * (1-valid_size))\n",
    "    valid_loss = valid_loss/(len(valid_loader.dataset) * valid_size)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(net.state_dict(), ckpt_name)\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lossl = [2222.424191, 2204.462145, 2176.80634, 2151.847436, 2124.632558, 2097.601839, 2074.707306, 2045.781735, 2018.009762,1993.408975]\n",
    "\n",
    "valid_lossl = [2.302149, 2.301881, 2.301242, 2.301733, 2.299262, 2.293659, 2.226565, 1.315879, 0.684092, 0.455054]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(train_lossl, columns=['train_loss'])\n",
    "df['valid_loss'] = valid_lossl\n",
    "df['train_loss'].plot(title='Training Loss')\n",
    "# df['valid_loss'].plot(title='Valid Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model_alexnet_CIFAR10_bayesian_v6.pt'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(output.shape)\n",
    "ckpt_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(ckpt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_uncertainity_softmax(output):\n",
    "    prediction = F.softmax(output, dim = 1)\n",
    "    results = torch.max(prediction, 1 )\n",
    "    p_hat = np.array(results[0].detach())\n",
    "    epistemic = np.mean(p_hat ** 2, axis=0) - np.mean(p_hat, axis=0) ** 2\n",
    "    epistemic += epistemic \n",
    "    #print (epistemic)\n",
    "    aleatoric = np.mean(p_hat * (1-p_hat), axis = 0)\n",
    "    aleatoric += aleatoric\n",
    "    #print (aleatoric)\n",
    "    return epistemic, aleatoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_uncertainty_normalized(output):\n",
    "    prediction = F.relu(output)\n",
    "    prediction = normalization_function(prediction)\n",
    "    results = torch.max(prediction, 1 )\n",
    "    p_hat = np.array(results[0].cpu().detach().numpy())\n",
    "#     print(p_hat)\n",
    "    epistemic = np.mean(p_hat ** 2, axis=0) - np.mean(p_hat, axis=0) ** 2 + 1e-10\n",
    "    epistemic += epistemic \n",
    "#     print (\"e: \" + str(epistemic))\n",
    "    aleatoric = np.mean(p_hat * (1-p_hat), axis = 0) + 1e-10\n",
    "    \n",
    "    aleatoric += aleatoric\n",
    "    return epistemic, aleatoric\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_function(x):\n",
    "    return (x) / torch.sum(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6024"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # track test loss\n",
    "# test_loss = 0.0\n",
    "# class_correct = list(0. for i in range(num_samples))\n",
    "# class_total = list(0. for i in range(10))\n",
    "\n",
    "output_class = np.asarray([])\n",
    "\n",
    "num_samples = 50\n",
    "net.eval()\n",
    "m = math.ceil(len(valid_loader_ooc.dataset) / batch_size)\n",
    "target_list = []\n",
    "# iterate over test data\n",
    "for batch_idx, (data, target) in enumerate(valid_loader_ooc):\n",
    "    target_list.append(target)\n",
    "    data = data.view(-1, inputs, resize, resize).repeat(num_samples, 1, 1, 1)\n",
    "    target = target.repeat(num_samples)\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if use_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    \n",
    "    if beta_type is \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "    elif cf.beta_type is \"Soenderby\":\n",
    "        beta = min(epoch / (cf.num_epochs // 4), 1)\n",
    "    elif cf.beta_type is \"Standard\":\n",
    "        beta = 1 / m\n",
    "    else:\n",
    "        beta = 0\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output, kl = net.probforward(data)\n",
    "    output = F.softmax(output, dim = 1)\n",
    "    \n",
    "#     if not torch.eq(torch.isnan(output).sum(), 0):\n",
    "#         print(output, kl)\n",
    "#         continue\n",
    "    \n",
    "    # calculate the batch loss\n",
    "    #loss = vi(output, target, kl, beta)\n",
    "#     print\n",
    "    # update test loss \n",
    "    #test_loss += loss.item()*data.size(0) / num_samples\n",
    "    #test_loss += loss.item()\n",
    "    # convert output probabilities to predicted class\n",
    "#     _, pred = torch.max(output, 1) \n",
    "    out_list = np.zeros((batch_size,outputs))\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i*batch_size, (i+1)*batch_size):\n",
    "            out_list[j%batch_size] += output.detach().cpu().numpy()[j]\n",
    "    out_list = out_list/num_samples\n",
    "    \n",
    "    max_val = np.amax(out_list, axis = 1) \n",
    "    max_class = np.argmax(out_list, axis = 1)\n",
    "    max_class[max_val<=0.3] = 10\n",
    "    output_class = np.append(output_class, max_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     33.303571\n",
       "6.0     25.535714\n",
       "2.0     17.901786\n",
       "7.0      9.330357\n",
       "10.0     7.857143\n",
       "8.0      5.357143\n",
       "3.0      0.625000\n",
       "5.0      0.089286\n",
       "dtype: float64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.unique(output_class, return_counts=True)\n",
    "import pandas as pd\n",
    "pd.Series(output_class).value_counts()*100/ (np.sum(pd.Series(output_class).value_counts()))\n",
    "# output_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# # track test loss\n",
    "# test_loss = 0.0\n",
    "# class_correct = list(0. for i in range(num_samples))\n",
    "# class_total = list(0. for i in range(10))\n",
    "\n",
    "output_class = np.asarray([])\n",
    "counter = 0\n",
    "num_samples = 10\n",
    "net.eval()\n",
    "m = math.ceil(len(valid_loader.dataset) / batch_size)\n",
    "target_list = []\n",
    "# iterate over test data\n",
    "for batch_idx, (data, target) in enumerate(valid_loader):\n",
    "    target_list.append(target)\n",
    "    data = data.view(-1, inputs, resize, resize).repeat(num_samples, 1, 1, 1)\n",
    "    target1 = target\n",
    "    target = target.repeat(num_samples)\n",
    "    # move tensors to GPU if CUDA is available\n",
    "    if use_cuda:\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output, kl = net.probforward(data)\n",
    "    output = F.softmax(output, dim = 1)\n",
    "    out_list = np.zeros((batch_size,outputs))\n",
    "    for i in range(num_samples):\n",
    "        for j in range(i*batch_size, (i+1)*batch_size):\n",
    "            out_list[j%batch_size] += output.detach().cpu().numpy()[j]\n",
    "    out_list = out_list/num_samples\n",
    "    \n",
    "    max_val = np.amax(out_list, axis = 1) \n",
    "    max_class = np.argmax(out_list, axis = 1)\n",
    "    max_class[max_val<=0.7] = 10\n",
    "    correct = (max_class == target1.detach().cpu().numpy())\n",
    "    counter += np.sum(correct)\n",
    "    output_class = np.append(output_class, max_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8164010624169987"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter/len(valid_loader.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0     17.812085\n",
       "5.0     11.802789\n",
       "3.0     11.420983\n",
       "2.0     11.321381\n",
       "6.0     10.856574\n",
       "10.0    10.790173\n",
       "7.0      9.495352\n",
       "4.0      8.549137\n",
       "8.0      7.951527\n",
       "dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(output_class).value_counts()*100/ (np.sum(pd.Series(output_class).value_counts()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
