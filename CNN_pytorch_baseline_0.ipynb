{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "View more, visit my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "My Youtube Channel: https://www.youtube.com/user/MorvanZhou\n",
    "\n",
    "Dependencies:\n",
    "torch: 0.4\n",
    "torchvision\n",
    "matplotlib\n",
    "\"\"\"\n",
    "# library\n",
    "# standard library\n",
    "import os\n",
    "\n",
    "# third-party library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_size=(28,28), batch_size=64, root=\"../datasets/MainImageFolder\"):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    train_set = datasets.ImageFolder(root=root, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    return train_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_images(root='../../state-farm-distracted-driver-detection/imgs/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "batch_size = 64\n",
    "num_workers = 0\n",
    "\n",
    "train_ld = load_images(root='./train/')\n",
    "\n",
    "train_data = train_ld.dataset\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "\n",
    "valset= valid_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22424\n",
      "2489\n",
      "13913\n",
      "13913\n",
      "6022\n",
      "2489\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 64\n",
    "# train_ld = load_images(root='./train/')\n",
    "# train_ld1 = load_images(root='./train/')\n",
    "\n",
    "# print(len(train_ld.dataset))\n",
    "# validation_split = 0.3\n",
    "# shuffle_dataset = True\n",
    "# random_seed= 42\n",
    "\n",
    "# # Creating data indices for training and validation splits:\n",
    "# dataset_size = len(train_ld.dataset)\n",
    "# indices = list(range(dataset_size))\n",
    "# split = int(np.floor(validation_split * dataset_size))\n",
    "# if shuffle_dataset :\n",
    "#     np.random.seed(random_seed)\n",
    "#     np.random.shuffle(indices)\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# def get_same_index(target, label):\n",
    "#     label_indices = []\n",
    "    \n",
    "#     for i in range(len(target)):\n",
    "#         if target[i][1] == label:\n",
    "#             label_indices.append(i)\n",
    "\n",
    "#     return label_indices\n",
    "\n",
    "# ind = get_same_index(train_ld.dataset.samples,0)\n",
    "# sampler_chk1 = torch.utils.data.sampler.SubsetRandomSampler(ind)\n",
    "\n",
    "# print(len(sampler_chk1))\n",
    "\n",
    "# train_idx1 = list(set(train_idx) - set(sampler_chk1))\n",
    "# print(len(train_idx1))\n",
    "\n",
    "# valid_idx1 = list(set(valid_idx) - set(sampler_chk1))\n",
    "# len(valid_idx1)\n",
    "\n",
    "# train_data = train_ld.dataset\n",
    "# train_data1 = train_ld1.dataset\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(train_idx1)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx1)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "#     sampler=train_sampler)\n",
    "# print(len(train_loader.sampler))\n",
    "# valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "#     sampler=valid_sampler)\n",
    "# print(len(valid_loader.sampler))\n",
    "# valid_loader_ooc = torch.utils.data.DataLoader(train_data1, batch_size=batch_size, \n",
    "#     sampler=sampler_chk1)\n",
    "# print(len(valid_loader_ooc.sampler))\n",
    "\n",
    "# train_data=train_loader.dataset\n",
    "# valid_data=valid_loader.dataset\n",
    "# valset=valid_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insipired on LeNet, first proposed in: \n",
    "# https://ieeexplore.ieee.org/abstract/document/726791\n",
    "# PDF: http://www.cs.virginia.edu/~vicente/deeplearning/readings/lecun1998.pdf\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        # Convolutional layers.\n",
    "        self.conv1 = nn.Conv2d(3, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        \n",
    "        # Linear layers.\n",
    "        self.fc1 = nn.Linear(64*4*4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv1 + ReLU + MaxPooling.\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        \n",
    "        # Conv2 + ReLU + MaPooling.\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        \n",
    "        # This flattens the output of the previous layer into a vector.\n",
    "        out = out.view(out.size(0), -1) \n",
    "        \n",
    "        # Linear layer + ReLU.\n",
    "        out = F.relu(self.fc1(out))\n",
    "        # Linear layer + ReLU.\n",
    "        out = F.relu(self.fc2(out))\n",
    "        # A final linear layer at the end.\n",
    "        out = self.fc3(out)\n",
    "       \n",
    "        # We will not add Softmax here because nn.CrossEntropy does it.\n",
    "        # Read the documentation for nn.CrossEntropy.\n",
    "        return out\n",
    "\n",
    "model = LeNet()\n",
    "\n",
    "model = model.cuda()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = loss_fn.cuda()\n",
    "correct = 0.0\n",
    "cum_loss = 0.0\n",
    "\n",
    "learningRate = 5e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr = learningRate, \n",
    "                      momentum = 0.9, weight_decay = 1e-4)\n",
    "N = 5\n",
    "# log accuracies and losses.\n",
    "train_accuracies = []; val_accuracies = []\n",
    "train_losses = []; val_losses = []\n",
    "batchSize=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-epoch 0. Iteration 00100, Avg-Loss: 0.0574, Accuracy: 0.9830\n",
      "Train-epoch 0. Iteration 00200, Avg-Loss: 0.0558, Accuracy: 0.9834\n",
      "Validation-epoch 0. Avg-Loss: 0.1105, Accuracy: 0.9672\n",
      "Train-epoch 1. Iteration 00100, Avg-Loss: 0.0846, Accuracy: 0.9736\n",
      "Train-epoch 1. Iteration 00200, Avg-Loss: 0.0881, Accuracy: 0.9739\n",
      "Validation-epoch 1. Avg-Loss: 0.0722, Accuracy: 0.9779\n",
      "Train-epoch 2. Iteration 00100, Avg-Loss: 0.0225, Accuracy: 0.9936\n",
      "Train-epoch 2. Iteration 00200, Avg-Loss: 0.0280, Accuracy: 0.9918\n",
      "Validation-epoch 2. Avg-Loss: 0.0768, Accuracy: 0.9810\n",
      "Train-epoch 3. Iteration 00100, Avg-Loss: 0.0232, Accuracy: 0.9925\n",
      "Train-epoch 3. Iteration 00200, Avg-Loss: 0.0369, Accuracy: 0.9884\n",
      "Validation-epoch 3. Avg-Loss: 0.1520, Accuracy: 0.9636\n",
      "Train-epoch 4. Iteration 00100, Avg-Loss: 0.0321, Accuracy: 0.9898\n",
      "Train-epoch 4. Iteration 00200, Avg-Loss: 0.0333, Accuracy: 0.9893\n",
      "Validation-epoch 4. Avg-Loss: 0.0725, Accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, N):\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "\n",
    "      # Make a pass over the training data.\n",
    "        model.train()\n",
    "        total_train=0\n",
    "        for (i, (inputs, labels)) in enumerate(train_loader):\n",
    "            inputs = inputs.cuda()\n",
    "#             print(inputs.shape)\n",
    "            labels = labels.cuda()\n",
    "            total_train+=len(inputs)\n",
    "            # Forward pass. (Prediction stage)\n",
    "            scores = model(inputs)\n",
    "            loss = loss_fn(scores, labels)\n",
    "\n",
    "            # Count how many correct in this batch.\n",
    "            max_scores, max_labels = scores.max(1)\n",
    "            correct += (max_labels == labels).sum().item()\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            # Zero the gradients in the network.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass. (Gradient computation stage)\n",
    "            loss.backward()\n",
    "\n",
    "            # Parameter updates (SGD step) -- if done with torch.optim!\n",
    "            optimizer.step()\n",
    "\n",
    "            # Parameter updates (SGD step) -- if done manually!\n",
    "            # for param in model.parameters():\n",
    "            #   param.data.add_(-learningRate, param.grad)\n",
    "\n",
    "            # Logging the current results on training.\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print('Train-epoch %d. Iteration %05d, Avg-Loss: %.4f, Accuracy: %.4f' % (epoch, i + 1, cum_loss / (i + 1), correct / ((i + 1) * batchSize)))\n",
    "\n",
    "        train_accuracies.append(correct / total_train)\n",
    "        train_losses.append(cum_loss / (i + 1))   \n",
    "\n",
    "        # Make a pass over the validation data.\n",
    "        correct = 0.0\n",
    "        cum_loss = 0.0\n",
    "        model.eval()\n",
    "        total=0\n",
    "        for (i, (inputs, labels)) in enumerate(valid_loader):\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            total += len(inputs)\n",
    "            # Forward pass. (Prediction stage)\n",
    "            scores = model(inputs)\n",
    "            cum_loss += loss_fn(scores, labels).item()\n",
    "\n",
    "            # Count how many correct in this batch.\n",
    "            max_scores, max_labels = scores.max(1)\n",
    "            correct += (max_labels == labels).sum().item()\n",
    "\n",
    "        val_accuracies.append(correct / total)\n",
    "        val_losses.append(cum_loss / (i + 1))\n",
    "\n",
    "        # Logging the current results on validation.\n",
    "        print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "            (epoch, cum_loss / (i + 1), correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.0 64\n",
      "123.0 128\n",
      "185.0 192\n",
      "247.0 256\n",
      "310.0 320\n",
      "373.0 384\n",
      "432.0 448\n",
      "493.0 512\n",
      "556.0 576\n",
      "619.0 640\n",
      "680.0 704\n",
      "742.0 768\n",
      "804.0 832\n",
      "863.0 896\n",
      "925.0 960\n",
      "988.0 1024\n",
      "1051.0 1088\n",
      "1114.0 1152\n",
      "1174.0 1216\n",
      "1236.0 1280\n",
      "1300.0 1344\n",
      "1363.0 1408\n",
      "1425.0 1472\n",
      "1485.0 1536\n",
      "1547.0 1600\n",
      "1606.0 1664\n",
      "1667.0 1728\n",
      "1730.0 1792\n",
      "1791.0 1856\n",
      "1855.0 1920\n",
      "1918.0 1984\n",
      "1982.0 2048\n",
      "2044.0 2112\n",
      "2108.0 2176\n",
      "2171.0 2240\n",
      "2234.0 2304\n",
      "2297.0 2368\n",
      "2361.0 2432\n",
      "2424.0 2496\n",
      "2487.0 2560\n",
      "2551.0 2624\n",
      "2612.0 2688\n",
      "2676.0 2752\n",
      "2740.0 2816\n",
      "2804.0 2880\n",
      "2865.0 2944\n",
      "2928.0 3008\n",
      "2990.0 3072\n",
      "3053.0 3136\n",
      "3116.0 3200\n",
      "3177.0 3264\n",
      "3237.0 3328\n",
      "3300.0 3392\n",
      "3364.0 3456\n",
      "3424.0 3520\n",
      "3486.0 3584\n",
      "3548.0 3648\n",
      "3609.0 3712\n",
      "3672.0 3776\n",
      "3734.0 3840\n",
      "3796.0 3904\n",
      "3857.0 3968\n",
      "3917.0 4032\n",
      "3980.0 4096\n",
      "4042.0 4160\n",
      "4104.0 4224\n",
      "4167.0 4288\n",
      "4229.0 4352\n",
      "4291.0 4416\n",
      "4354.0 4480\n",
      "4358.0 4484\n",
      "4358.0 22424\n",
      "Validation-epoch 4. Avg-Loss: 0.0969, Accuracy: 0.9719\n"
     ]
    }
   ],
   "source": [
    "# Make a pass over the validation data.\n",
    "correct = 0.0\n",
    "cum_loss = 0.0\n",
    "model.eval()\n",
    "total=0\n",
    "for (i, (inputs, labels)) in enumerate(valid_loader):\n",
    "    inputs = inputs.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    # Forward pass. (Prediction stage)\n",
    "    scores = model(inputs)\n",
    "    cum_loss += loss_fn(scores, labels).item()\n",
    "\n",
    "    # Count how many correct in this batch.\n",
    "    max_scores, max_labels = scores.max(1)\n",
    "    correct += (max_labels == labels).sum().item()\n",
    "    total+= len(inputs)\n",
    "    print(correct, total)\n",
    "\n",
    "print(correct, len(valid_data))\n",
    "\n",
    "val_accuracies.append(correct/total)\n",
    "val_losses.append(cum_loss / (i + 1))\n",
    "\n",
    "# Logging the current results on validation.\n",
    "print('Validation-epoch %d. Avg-Loss: %.4f, Accuracy: %.4f' % \n",
    "    (epoch, cum_loss / (i + 1), correct/total ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(valid_loader.sampler)\n",
    "valid_loader.batch_size*len(valid_loader)\n",
    "valid_loader.batch_sampler.batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
